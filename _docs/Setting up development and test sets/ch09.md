---
title: 优化指标和满意度指标
permalink: /docs/ch09/
---

下面将提到组合多个评估指标的另一种方法。

假设你既关心学习算法的准确率（accuracy），又在意其运行时间（running time），请从下面的三个分类器中进行选择：

| Classifier | Accuracy | Running time |
| ---------- | -------- | ------------ |
| A          | 90%      | 80ms         |
| B          | 92%      | 95ms         |
| C          | 95%      | 1,500ms      |

将准确率和与运行时间放入单个公式计算后可以导出单个的指标，这似乎不太自然，例如：

$$
Accuracy - 0.5 * RunningTime
$$

你可以有一种替代的方案：首先定义一个 “可接受的” 运行时间，一般低于 100ms 。接着在限定的运行时间范围内尝试最大化分类器的准确率。此处的运行时间则是一种 “满意度指标”  —— 你的分类器必须在这个指标上表现得 “足够好” ，这儿指的是它的上限是 100ms；而准确度则是一种 “优化指标”。

如果考虑 $ N $ 项不同的标准，比如模型的二进制文件大小（这对移动端 app 尤为重要，因为用户不想下载体积很大的 app）、运行时间和准确率。你或许会考虑设置 $ N-1 $ 个 “满意度” 指标，即要求它们满足一定的值或范围，下一步才是定义一个 “优化” 指标。例如分别为二进制文件的大小和运行时间设定可接受的阈值，并尝试根据这些限制来优化准确率指标。

最后再举一个例子，假设你正在设计一个硬件设备，该设备可以根据用户设置的特定 “唤醒词” 来唤醒系统，类似于 Amazon Echo 监听词为 “Alexa”，苹果（Apple） Siri 监听词为 “Hey Siri”，安卓（Android） 监听词为 “Okay Google”，以及百度（Baidu）应用监听 “Hello Baidu.” 我们关心的指标是是假正例率（false positive rate）—— 用户没有说出唤醒词，系统却被唤醒了，以及假反例率（false negative rate）——用户说出了唤醒词，系统却没能正确被唤醒。这个系统的一个较为合理的优化对象是尝试去最小化假反例率（优化指标），减少用户说出唤醒词而系统却没正确唤醒的发生率，同时受到每 24 小时不超过一次误报的约束（满意度指标）。

一旦你的团队在优化评估指标上保持一致，他们将能够取得更快的进展。
