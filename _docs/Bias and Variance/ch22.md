---
title: 与最优错误率比较
permalink: /docs/ch22/
---

在我们猫识别的例子中，“理想”错误率——即一个“最优”分类器可达到的值——接近 0%。人类几乎总是可以识别出图片中的猫。因此，我们希望机器能够做得同样好。

换作其他问题这将更难：假设你正在构建一个语音识别系统，并发现 14% 的音频片段有太多的背景噪声，或者太难以理解，导致即使是人类也无法识别出所说的内容。在这种情况下，即使是“最优”的语音识别系统也可能有约为 14% 的误差。

假设在这个语音识别问题上，你的算法达到：

- 训练错误率 = 15%
- 开发错误率 = 30%

算法在训练集上的表现已经接近最优错误率 14%，因此在偏差上或者说在训练集表现上没有太大的提升空间。然而，算法没有很好地泛化到开发集上，在方差造成的误差上还有很大的提升空间。

这个例子和前一章的第三个例子类似，都有 15% 的训练错误率和 30% 的开发错误率。如果最优错误率接近 0%，那么 15% 的训练错误率则留下了很大的提升空间，这表明降低偏差可能有益。但如果最优错误率是 14%，那么 15% 的训练错误率表现告诉我们，在分类器的偏差方面几乎没有改进的余地。

对于最佳错误率远超过零的问题，有一个对算法误差更详细的分解。继续使用上面我们语音识别的例子，可以将总的 30% 的开发集误差分解如下（类似的分析可以应用于测试集误差）：

- **最优错误率（“不可避免偏差”）**：14%。假设我们决定，即使是世界上最好的语音系统，仍会有 14% 的误差。我们可以将其认为是学习算法的偏差“不可避免”的部分。
- **可避免偏差**：1%。即训练错误率和最优误差率之间的差值。
- **方差**：15%。即开发错误和训练错误之间的差值。

> 如果可避免偏差值是负的，即你在训练集上的表现比最优错误率要好。这意味着你正在过拟合训练集，并且算法已经过度记忆（over-memorized）训练集。你应该专注于减少方差的方法，而不是进一步减少偏差的方法。

为了将这与我们之前的定义联系起来，偏差和可避免偏差关系如下：

偏差 = 最佳误差率（“不可避免偏差”）+ 可避免的偏差

> 使用这些定义是为了更好地传达关于如何改进学习算法的理解。这些定义与统计学家定义偏差和方差不同。从技术角度上说，这里定义的“偏差”应该叫做“我们认为是偏差的误差”；另外“可避免偏差”应该叫做“我们认为学习算法的偏差超过最优错误率的误差”。

这个“可避免偏差”反映了算法在训练集上的表现比“最优分类器”差多少。

方差的概念和之前保持一致。理论上来说，我们可以通过训练一个大规模训练集将方差减少到接近零。因此只要拥有足够大的数据集，所有的方差都是可以“避免的”，所以不存在所谓的“不可避免方差”。

再考虑一个例子，该例子中最优错误率是 14%，我们有：

- 训练误差 = 15%
- 开发误差 = 16%

我们在前一章称之为高偏差分类器，现在可避免偏差的误差是 1%，方差误差约为 1%。因此，算法已经做的很好了，几乎没有提升的空间。它只比最佳错误率差 2%。

从这些例子中我们可以看出，了解最优错误率有利于指导我们的后续步骤。在统计学上，最优错误率也被称为**贝叶斯错误率（Bayes error rate）**，或贝叶斯率。

如何才能知道最优错误率是多少呢？对于人类擅长的任务，例如识别图片或转录音频剪辑，你可以让普通人提供标签，然后测量这些人为标签相对于训练集标签的精度，这将给出最优错误率的估计。如果你正在解决甚至人也很难解决的问题（例如预测推荐什么电影，或向用户展示什么广告），这将很难去估计最优错误率。

在“与人类表现比较”（第33~35章）这一节中，我将更详细地讨论学习算法的表现和人类表现相比较的过程。

在前面几个章节中，你学习了如何通过查看训练集和开发集的错误率来估计可避免/不可避免的偏差和方差。下一章将讨论如何根据对此的理解来考虑优先减少偏差还是减少方差。项目当前的问题是高偏差（可避免偏差）还是高方差，将导致你应用截然不同的技术。请继续阅读。
