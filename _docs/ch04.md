---
title: 规模驱动机器学习发展
permalink: /docs/ch04/
---

关于深度学习（神经网络）的一些想法已经存在了数十年，而为什么它们到现在才流行起来了呢？

推动其近期发展的主要因素有两个：

- **数据可用性（data availability）**：如今人们在数字设备（笔记本电脑、移动设备等）上花费的时间越来越多，对应的数字化行为与活动产生了海量的数据，而这些数据都可以提供给我们的学习算法用来训练。
- **计算规模（computational scale）**：在近些年前，我们才开始能够使用现有的海量数据集来训练规模足够大的神经网络。

具体来说，即使你积累了更多的数据，但应用在类似于对数几率回归（logistic regression）这样的旧学习算法上，其性能表现（performance）也将趋于”平稳“。这意味着算法的学习曲线将”变得平缓“，就算提供更多的数据，算法的性能也将停止提升。

<img src="{{"ch04_01.png " | prepend: site.imgurl }}">

旧的学习算法似乎并不知道要如何来处理如今这个规模量级的数据。

如果你在相同的监督学习任务上选择训练出一个小型的神经网络（neutral network, NN），则有可能会获得较好的性能表现：

<img src="{{"ch04_02.png " | prepend: site.imgurl }}">

> 该图显示了在小数据集上应用神经网络的效果会更好，但这种效果与将神经网络应用在大数据集时不太一致。 在小数据集条件下，传统算法是否会表现得更好，取决于人们如何进行特征工程。例如，假设你只有 20 个训练样本，那么使用对数几率回归还是神经网络可能无关紧要；此时人为的特征工程将比让算法进行选择产生更大的影响。但如果你有 100 万个样本数据，我会赞同使用神经网络。

这里的”小型神经网络“指的是只含有少量的隐藏元/层/参数的神经网络。但如果你训练的神经网络规模越来越大，反而能够获得更好的表现：

<img src="{{"ch04_03.png " | prepend: site.imgurl }}">

因此，为了获得最佳的性能表现，你可以这样做：

1. 训练大型的神经网络，效果如同上图的绿色曲线；

2. 拥有海量的数据。

在算法训练时，许多其它的细节也同等重要，例如神经网络的架构。但目前来说，提升算法性能的更加可靠的方法仍然是训练更大的网络以及获取更多的数据。完成 1 和 2 的过程异常复杂，本书将对其中的细节作进一步的讨论。我们将从传统学习算法与神经网络中都起作用的通用策略入手，循序渐进地讲解至最前沿的构建深度学习系统的策略。
